{"cells":[{"cell_type":"markdown","metadata":{"id":"-eMoRFzE7sVf"},"source":["## Training for ResNet18\n","\n","**[replica]**: to examine performance after evaluator fixes, and obtain state dictionary"]},{"cell_type":"markdown","metadata":{},"source":["*if using google drive, run following two cells*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2340,"status":"ok","timestamp":1710388655459,"user":{"displayName":"Haoyu Zhou","userId":"17886401950307679344"},"user_tz":-660},"id":"mp1vAm1j79O0","outputId":"828a6a25-676c-4808-ebd0-914c96861b46"},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","project_dir = '/content/drive/MyDrive/master_courses/BIDH5001 Capstone/Project/\\\n","deep-classification'\n","os.chdir(project_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4764,"status":"ok","timestamp":1710387066856,"user":{"displayName":"Haoyu Zhou","userId":"17886401950307679344"},"user_tz":-660},"id":"BkyMECYW9UXl","outputId":"3964ab35-d720-494f-ced5-9a5919a582e1"},"outputs":[],"source":["%pip install pydicom    "]},{"cell_type":"markdown","metadata":{},"source":["imports, initiating dataloaders"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3662,"status":"ok","timestamp":1710388660985,"user":{"displayName":"Haoyu Zhou","userId":"17886401950307679344"},"user_tz":-660},"id":"gZ4IgZNy7sVj"},"outputs":[],"source":["# imports\n","import datetime\n","import time\n","import warnings\n","import torch\n","import config\n","from dataio.dataloader import create_dataloader\n","from networks.resnet_classifier import resnet18_classifier\n","from training.evaluation import AccuracyEvaluator, LossEvaluator\n","from training.utility.progress_bar import ProgressBar\n","from training.utility.early_stopper import ValLoss as EarlyStopper\n","\n","# whenever possible, use cuda instead of cpu\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","    warnings.warn(\"cuda is using CPU, this can be very slow\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# paths to save and load state dictionary\n","import os\n","import re\n","\n","load_sd = False\n","save_sd = False\n","\n","bin_path = \"bin\"\n","assert os.path.isdir(bin_path)\n","sd_files = [\n","    filename for filename in os.listdir(bin_path)\n","    if re.match(r'res18_state_[0-9]{2}-[0-9]{2}\\.pkl', filename)]\n","sd_files.sort(reverse=True)\n","sd_load_filename = sd_files[0] if len(sd_files) > 0 else None\n","sd_save_filename = f\"res18_state_{datetime.datetime.now().strftime('%m-%d')}.pkl\"\n","sd_load_path = os.path.join(bin_path, sd_load_filename) \\\n","    if sd_load_filename is not None else None\n","sd_save_path = os.path.join(bin_path, sd_save_filename)"]},{"cell_type":"markdown","metadata":{},"source":["preparing the training:\n","1. read dicoms and labels from configuration and tracking table, initiate dataloaders\n","2. set up criterions and optimizers, training parameters"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# reading from configurations\n","dicoms = config.tracking_table['dicom_path'].to_list()\n","labels = config.tracking_table['label'].astype('int16').to_list()\n","\n","# initiate the dataloaders\n","dataloader_dict = create_dataloader(\n","    dicoms, labels,\n","    dicom_dir = config.dicom_dir,\n","    batch_size = 8,\n","    validation_size = config.validation_size,\n","    test_size = config.test_size,\n","    img_size = (128, 128) # (64, 64)\n",")\n","training_dataloader, validation_dataloader, test_dataloader = \\\n","    (dataloader_dict.get(key) for key in ('training_dataloader', 'validation_dataloader', 'test_dataloader'))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":694,"status":"ok","timestamp":1710388665235,"user":{"displayName":"Haoyu Zhou","userId":"17886401950307679344"},"user_tz":-660},"id":"mNMjQKN57sVk"},"outputs":[],"source":["# some settings, move to config in future\n","learning_rate = 0.00005 # 0.00005, originally without lr scheduler\n","n_epoches = 15\n","min_epoches = 7\n","starting_epoch = 0\n","\n","# the classifier model\n","model = resnet18_classifier\n","# load state dict into \n","if load_sd and sd_load_path:\n","    state_dict = torch.load(sd_load_path)\n","    model.load_state_dict(state_dict)\n","\n","# optimizers\n","loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","#     optimizer, max_lr=learning_rate,\n","#     steps_per_epoch=len(training_dataloader), epochs=n_epoches,\n","#     )\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, 'min', factor=0.25, patience=0,\n","    # threshold=0.05, threshold_mode='rel',\n","    min_lr = learning_rate / 32,\n","    cooldown=7,\n",")\n","earlystopper = EarlyStopper(tolerance=3, target=0.25, delta=0.005)\n","\n","# accuracy and loss evaluators\n","training_loss = LossEvaluator(loss_function)\n","validation_loss = LossEvaluator(loss_function)\n","validation_accuracy = AccuracyEvaluator()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# to continue training for n_epoches\n","n_epoches = 15"]},{"cell_type":"markdown","metadata":{"id":"lj81jJj17sVl"},"source":["training for resnet18 image quality classifier"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch # 1\n","train loss: 0.682          validation loss: 0.845          validation accuracy: 78.57 %\n","precision: -1          recall: -1          f1: -1.0\n","duration: 29.36 s          learning rate: 5e-05\n","---------------------------------------------------------------------------\n","epoch # 2\n","train loss: 0.183          validation loss: 0.848          validation accuracy: 78.57 %\n","precision: -1          recall: -1          f1: -1.0\n","duration: 29.06 s          learning rate: 1.25e-05\n","---------------------------------------------------------------------------\n","epoch # 3\n","train loss: 0.106          validation loss: 0.849          validation accuracy: 78.57 %\n","precision: -1          recall: -1          f1: -1.0\n","duration: 29.21 s          learning rate: 1.25e-05\n","---------------------------------------------------------------------------\n","epoch # 4\n","train loss: 0.064          validation loss: 0.848          validation accuracy: 78.57 %\n","precision: -1          recall: -1          f1: -1.0\n","duration: 29.26 s          learning rate: 1.25e-05\n","---------------------------------------------------------------------------\n","epoch # 5\n","train loss: 0.07          validation loss: 0.848          validation accuracy: 78.57 %\n","precision: -1          recall: -1          f1: -1.0\n","duration: 28.97 s          learning rate: 1.25e-05\n","---------------------------------------------------------------------------\n","epoch # 6\n","train loss: 0.058          validation loss: 0.846          validation accuracy: 78.57 %\n","precision: -1          recall: -1          f1: -1.0\n","duration: 29.12 s          learning rate: 1.25e-05\n","---------------------------------------------------------------------------\n","epoch # 7\n","train loss: 0.04          validation loss: 0.845          validation accuracy: 78.57 %\n","precision: -1          recall: -1          f1: -1.0\n","duration: 29.06 s          learning rate: 1.25e-05\n","---------------------------------------------------------------------------\n","epoch # 8\n","train loss: 0.037          validation loss: 0.843          validation accuracy: 78.57 %\n","precision: -1          recall: -1          f1: -1.0\n","duration: 29.04 s          learning rate: 1.25e-05\n","---------------------------------------------------------------------------\n","early stopper triggered, break\n"]}],"source":["# evaluators for training/validation loss and accuracy\n","# training_loss = LossEvaluator(loss_function)\n","# validation_loss = LossEvaluator(loss_function)\n","# validation_accuracy = AccuracyEvaluator()\n","\n","# training epoches loop\n","for epoch in range(n_epoches):\n","    current_epoch = epoch + 1 + starting_epoch\n","    print(f'epoch # {current_epoch}')\n","    progress = ProgressBar(len(training_dataloader) + len(validation_dataloader))\n","    epoch_start_time = time.time()\n","    # reset pred and actual labels after each epoch\n","    validation_accuracy.reset()\n","    training_loss.reset()\n","    validation_loss.reset()\n","    # training network\n","    for images, labels in training_dataloader:\n","        # images = images.to(torch.float32)\n","        # labels = labels.to(torch.float32)\n","        labels_pred = model(images)\n","        # train loss\n","        loss_training = loss_function(labels_pred, labels)\n","        training_loss.append_loss(loss_training.item())\n","        # backpropagation\n","        optimizer.zero_grad()\n","        loss_training.backward()\n","        optimizer.step()\n","        progress.step()\n","    # validating network\n","    for images, labels in validation_dataloader:\n","        # images = images.to(torch.float32)\n","        # labels = labels.to(torch.float32)\n","        labels_pred = model(images)\n","        loss_validation = loss_function(labels_pred, labels)\n","        validation_loss.append_loss(loss_validation.item())\n","        validation_accuracy.append(labels_pred, labels)\n","        progress.step()\n","    epoch_end_time = time.time()\n","    # step scheduler\n","    scheduler.step(validation_loss.value())\n","    # step early stopper\n","    earlystopper.step(validation_loss.value())\n","    print(\n","        f'train loss: {training_loss}  \\\n","        validation loss: {validation_loss}  \\\n","        validation accuracy: {validation_accuracy}')\n","    print(\n","        f'precision: {round(validation_accuracy.precision(), 3)}  \\\n","        recall: {round(validation_accuracy.recall(), 3)}  \\\n","        f1: {round(validation_accuracy.f1(), 3)}')\n","    # the evaluators have __str__\n","    print(\n","        f'duration: {round(epoch_end_time - epoch_start_time, 2)} s  \\\n","        learning rate: {round(scheduler.get_last_lr()[0], 9)}')\n","    print('-'*75)\n","    if earlystopper.stop() and current_epoch > min_epoches:\n","        print('early stopper triggered, break')\n","        break\n","    \n","\n","# update the starting epoch number\n","starting_epoch += epoch"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["[tensor([ 0.8357,  0.1737, -0.2336], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8104,  0.3573, -0.2860], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.7725,  0.2724, -0.3439], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.7991,  0.2672, -0.4124], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8815,  0.2368, -0.2031], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.7080,  0.2786, -0.2192], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.7891,  0.3612, -0.1691], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8260,  0.3809, -0.2872], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.7775,  0.1921, -0.3413], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.9571,  0.3687, -0.2879], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8966,  0.2854, -0.4026], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.7867,  0.3020, -0.1907], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8430,  0.2592, -0.2831], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8670,  0.3114, -0.3357], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8470,  0.2649, -0.2877], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.7899,  0.3393, -0.3058], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8909,  0.3617, -0.3253], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8784,  0.3620, -0.3914], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.9156,  0.4122, -0.3190], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8330,  0.2959, -0.2170], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8958,  0.4165, -0.3273], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8566,  0.4564, -0.3134], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.9574,  0.2739, -0.3014], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.7532,  0.3617, -0.2891], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8797,  0.2976, -0.2694], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.9361,  0.3052, -0.3285], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8563,  0.2856, -0.2688], grad_fn=<UnbindBackward0>),\n"," tensor([ 0.8786,  0.2797, -0.4137], grad_fn=<UnbindBackward0>)]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["validation_accuracy.labels_pred_raw"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["[1,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["validation_accuracy.labels_actual"]},{"cell_type":"markdown","metadata":{},"source":["save state dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save model state_dicts to target path\n","# if save_sd:\n","torch.save(model.state_dict(), sd_save_path)\n","print(f'state dict saved to path: {sd_save_path}')"]},{"cell_type":"markdown","metadata":{},"source":["statistics of the distributions of image quality labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_labels = []\n","for _, label in training_dataloader:\n","    training_labels.append(torch.argmax(label).item())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\n","    'total:', len(training_labels), '\\n',\n","    'class 1:', len([x for x in training_labels if x == 0]), '\\n',\n","    'class 2:', len([x for x in training_labels if x == 1]), '\\n',\n","    'class 3:', len([x for x in training_labels if x == 2]), '\\n',\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{},"source":["*(debugging the network)*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(validation_loss.labels_actual_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datetime\n","state_dict_save_path = f\"bin/res18_state_{datetime.datetime.now().strftime('%m-%d')}\"\n","state_dict_save_path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datetime\n","state_dict_save_path = f\"bin/res18_state_{datetime.datetime.now().strftime('%m-%d')}.pkl\"\n","torch.save(model.state_dict(), state_dict_save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_accuracy._tfpn()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sum([len(training_dataloader), len(validation_dataloader), len(test_dataloader)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_pred_labels = [torch.argmax(lbl).item() for lbl in validation_accuracy.labels_pred_raw]\n","validation_actual_labels = [torch.argmax(lbl).item() for lbl in validation_accuracy.labels_actual_raw]\n","sum(validation_pred_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_total = len(validation_actual_labels)\n","n_correct = 0\n","for pred_label, actual_label in zip(validation_pred_labels, validation_actual_labels):\n","    if pred_label == actual_label:\n","        n_correct += 1\n","n_correct / n_total"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_accuracy.labels_pred_raw[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_accuracy.accuracy()"]},{"cell_type":"markdown","metadata":{},"source":["calculate test set loss and accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# evaluators for test loss and accuracy\n","test_loss = LossEvaluator(criterion=loss_function)\n","test_accuracy = AccuracyEvaluator()\n","\n","# iterate through test dataloader\n","for images, labels in test_dataloader:\n","    # images = images.to(torch.float32)\n","    # labels = labels.to(torch.float32)\n","    labels_pred = model(images)\n","    loss = loss_function(labels_pred, labels)\n","    test_loss.append_loss(loss)\n","    test_accuracy.append(labels_pred, labels)\n","print(f'test loss: {test_loss}\\\n","    test accuracy: {test_accuracy}')\n","print(\n","    f'precision: {round(test_accuracy.precision(), 3)}  \\\n","    recall: {round(test_accuracy.recall(), 3)}  \\\n","    f1: {round(test_accuracy.f1(), 3)}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# function shows saliency map\n","def calculate_saliency_map(model, image, target_class):\n","\n","    \"\"\"\n","    Calculates the saliency map for a given image and target class.\n","\n","    Args:\n","    model: A PyTorch model.\n","    image: A PyTorch tensor containing the image.\n","    target_class: The target class.\n","\n","    Returns:\n","    numpy array containing the saliency map.\n","    \"\"\"\n","\n","    # Forward pass the image through the model.\n","    output = model(image)\n","    # Get the gradient of the output with respect to the input image.\n","    # gradient = torch.autograd.grad(output[0][target_class], image[0])\n","    gradient = torch.autograd.grad(\n","        output, image, grad_outputs=target_class.view(1,-1),\n","        allow_unused=True)[0][0]\n","    # Calculate the absolute value of the gradient.\n","    # saliency_map = torch.abs(gradient)\n","    saliency_map = torch.norm(gradient)\n","    # Normalize the saliency map.\n","    saliency_map = saliency_map / torch.max(saliency_map)\n","    saliency_map_np = saliency_map.detach().numpy()\n","    return saliency_map_np"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gradient = torch.autograd.grad(output, image, grad_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","image = images[0][0]\n","plt.imshow(image, cmap='binary_r')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tempfile\n","for images in test_dataloader.view()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(validation_accuracy.labels_actual_raw)"]},{"cell_type":"markdown","metadata":{},"source":["## Appendix\n","additional details about the model"]},{"cell_type":"markdown","metadata":{},"source":["structure of the resnet_18 model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["isinstance(model, torch.nn.Module)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
