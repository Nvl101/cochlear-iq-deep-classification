{"cells":[{"cell_type":"markdown","metadata":{"id":"-eMoRFzE7sVf"},"source":["## Training for ResNet18"]},{"cell_type":"markdown","metadata":{},"source":["*if using google drive, run following two cells*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2340,"status":"ok","timestamp":1710388655459,"user":{"displayName":"Haoyu Zhou","userId":"17886401950307679344"},"user_tz":-660},"id":"mp1vAm1j79O0","outputId":"828a6a25-676c-4808-ebd0-914c96861b46"},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","project_dir = '/content/drive/MyDrive/master_courses/BIDH5001 Capstone/Project/\\\n","deep-classificataon'\n","os.chdir(project_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4764,"status":"ok","timestamp":1710387066856,"user":{"displayName":"Haoyu Zhou","userId":"17886401950307679344"},"user_tz":-660},"id":"BkyMECYW9UXl","outputId":"3964ab35-d720-494f-ced5-9a5919a582e1"},"outputs":[],"source":["%pip install pydicom"]},{"cell_type":"markdown","metadata":{},"source":["imports, initiating dataloaders"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3662,"status":"ok","timestamp":1710388660985,"user":{"displayName":"Haoyu Zhou","userId":"17886401950307679344"},"user_tz":-660},"id":"gZ4IgZNy7sVj"},"outputs":[{"name":"stdout","output_type":"stream","text":["- starting from epoch: 1\n"]}],"source":["# imports\n","import datetime\n","import time\n","import tempfile\n","import warnings\n","import torch\n","import config\n","from dataio.dataloader import create_dataloader\n","from networks.resnet_classifier import resnet18_classifier\n","from training.evaluation import AccuracyEvaluator, LossEvaluator\n","from training.utility.progress_bar import ProgressBar\n","from training.utility.early_stopper import ValLoss as EarlyStopper\n","from training.utility.signal_control import SignalFileControl\n","import os\n","import re\n","\n","# whenever possible, use cuda instead of cpu\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","    warnings.warn(\"torch is using CPU, as cuda is unavailable. this is inefficient.\")\n","torch.set_default_device(device)\n","\n","# paths to save and load state dictionary\n","load_sd = False\n","save_sd = False\n","\n","bin_path = os.path.abspath(\"bin\")\n","assert os.path.isdir(bin_path)\n","\n","# if load_sd set True, load from state dict\n","sd_load_path = None\n","starting_epoch = 0\n","sd_file_pattern = r'resnet18_state_[0-9]{2}-[0-9]{2}-ep_[0-9]{2}\\.pkl'\n","sd_epoch_pattern = r'resnet18_state_[0-9]{2}-[0-9]{2}-ep_([0-9]{2})\\.pkl'\n","if load_sd:\n","    sd_files = [\n","        filename for filename in os.listdir(bin_path)\n","        if re.match(sd_file_pattern, filename)]\n","    sd_files.sort(reverse=True)\n","    if len(sd_files) > 0:\n","        sd_filename = sd_files[0]\n","        sd_load_path = os.path.join(bin_path, sd_filename) \\\n","            if sd_filename is not None else None\n","        match = re.search(sd_epoch_pattern, sd_filename)\n","        starting_epoch = int(match.group(1))\n","        print(f'- loaded state from: {sd_filename}')\n","print(f\"- starting from epoch: {starting_epoch + 1}\")\n","\n","# tempfile to save the model every time it reaches the best validation loss\n","state_dict_backup_path = tempfile.mktemp(prefix='state-dict_', suffix='.pth')"]},{"cell_type":"markdown","metadata":{},"source":["preparing the training:\n","1. read dicoms and labels from configuration and tracking table, initiate dataloaders\n","2. set up criterions and optimizers, training parameters"]},{"cell_type":"markdown","metadata":{},"source":["*the data*"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# reading from configurations\n","dicoms = config.dicom_paths # tracking_table['dicom_path'].to_list()\n","labels = config.labels\n","\n","# initiate the dataloaders\n","dataloader_dict = create_dataloader(\n","    dicoms, labels,\n","    batch_size = 8,\n","    validation_size = config.validation_size,\n","    test_size = config.test_size,\n","    img_size = (224, 224),\n","    use_3_channels = True,\n",")\n","training_dataloader, validation_dataloader, test_dataloader = \\\n","    (dataloader_dict.get(key) for key in ('training_dataloader', 'validation_dataloader', 'test_dataloader'))"]},{"cell_type":"markdown","metadata":{},"source":["*the model*"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":694,"status":"ok","timestamp":1710388665235,"user":{"displayName":"Haoyu Zhou","userId":"17886401950307679344"},"user_tz":-660},"id":"mNMjQKN57sVk"},"outputs":[],"source":["# some settings, move to config in future\n","learning_rate = 0.0001 # 0.00005, originally without lr scheduler\n","n_epoches = 7\n","min_epoches = 3 # minimum epoches before early stopping allowed\n","\n","# the classifier model\n","model = resnet18_classifier.to(device=device)\n","# load state dict into \n","if load_sd and sd_load_path:\n","    state_dict = torch.load(sd_load_path)\n","    model.load_state_dict(state_dict)\n","\n","# optimizers\n","loss_function = torch.nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","#     optimizer, max_lr=learning_rate,\n","#     steps_per_epoch=len(training_dataloader), epochs=n_epoches,\n","#     )\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, 'min', factor=0.25, patience=1,\n","    # threshold=0.05, threshold_mode='rel',\n","    min_lr = learning_rate / 32,\n","    cooldown=7,\n",")\n","\n","# accuracy and loss evaluators\n","training_loss_evaluator = LossEvaluator(loss_function)\n","validation_loss_evaluator = LossEvaluator(loss_function)\n","validation_accuracy_evaluator = AccuracyEvaluator()\n","\n","best_epoch, best_loss = None, None"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# to continue training for n_epoches\n","n_epoches = 10"]},{"cell_type":"markdown","metadata":{"id":"lj81jJj17sVl"},"source":["training for resnet18 image quality classifier"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["signal file path: C:\\Users\\user\\AppData\\Local\\Temp\\signal-05-14-o80lrdi0\n","epoch # 1\n","train loss: 0.186          validation loss: 0.69          validation accuracy: 9.24 %\n","precision: 0.118          recall: 1.0          f1: 0.211\n","duration: 30.44 s          learning rate: 0.0001\n","---------------------------------------------------------------------------\n","epoch # 2\n","train loss: 0.049          validation loss: 0.649          validation accuracy: 9.24 %\n","precision: 0.118          recall: 1.0          f1: 0.211\n","duration: 29.38 s          learning rate: 0.0001\n","---------------------------------------------------------------------------\n","epoch # 3\n","train loss: 0.024          validation loss: 0.503          validation accuracy: 54.62 %\n","precision: 0.1          recall: 0.357          f1: 0.156\n","duration: 29.2 s          learning rate: 0.0001\n","---------------------------------------------------------------------------\n","epoch # 4\n","train loss: 0.016          validation loss: 0.456          validation accuracy: 88.24 %\n","precision: -1          recall: 0.0          f1: -1\n","duration: 28.42 s          learning rate: 0.0001\n","---------------------------------------------------------------------------\n","epoch # 5\n","train loss: 0.041          validation loss: 0.421          validation accuracy: 88.24 %\n","precision: -1          recall: 0.0          f1: -1\n","duration: 27.8 s          learning rate: 0.0001\n","---------------------------------------------------------------------------\n","epoch # 6\n","train loss: 0.013          validation loss: 0.38          validation accuracy: 88.24 %\n","precision: -1          recall: 0.0          f1: -1\n","duration: 28.1 s          learning rate: 0.0001\n","---------------------------------------------------------------------------\n","epoch # 7\n","train loss: 0.005          validation loss: 0.358          validation accuracy: 88.24 %\n","precision: -1          recall: 0.0          f1: -1\n","duration: 27.45 s          learning rate: 0.0001\n","---------------------------------------------------------------------------\n","---------------------------------------------------------------------------\n","best validation loss: 0.35774934316883567, epoch 7\n","saved to temp file: C:\\Users\\user\\AppData\\Local\\Temp\\state-dict_tsbojvvu.pth\n"]}],"source":["# evaluators for training/validation loss and accuracy\n","# training_loss = LossEvaluator(loss_function)\n","# validation_loss = LossEvaluator(loss_function)\n","# validation_accuracy = AccuracyEvaluator()\n","earlystopper = EarlyStopper(tolerance=4, target=0.25)\n","signalstopper = SignalFileControl()\n","\n","# training epoches loop\n","for epoch in range(n_epoches):\n","    current_epoch = epoch + 1 + starting_epoch\n","    print(f'epoch # {current_epoch}')\n","    progress = ProgressBar(len(training_dataloader) + len(validation_dataloader))\n","    epoch_start_time = time.time()\n","    # reset pred and actual labels after each epoch\n","    validation_accuracy_evaluator.reset()\n","    training_loss_evaluator.reset()\n","    validation_loss_evaluator.reset()\n","    \n","    # training network\n","    for images, labels in training_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        labels_pred = model(images)\n","        # train loss\n","        training_loss = loss_function(labels_pred, labels)\n","        # backpropagation\n","        optimizer.zero_grad()\n","        training_loss.backward()\n","        optimizer.step()\n","        progress.step()\n","        training_loss_evaluator.append_loss(training_loss.item())\n","    \n","    # validating network\n","    for images, labels in validation_dataloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        labels_pred = model(images)\n","        validation_loss = loss_function(labels_pred, labels)\n","        progress.step()\n","        validation_loss_evaluator.append_loss(validation_loss.item())\n","        validation_accuracy_evaluator.append(labels_pred, labels)\n","    epoch_end_time = time.time()\n","    # step scheduler\n","    scheduler.step(validation_loss_evaluator.value())\n","    print(\n","        f'train loss: {training_loss_evaluator}  \\\n","        validation loss: {validation_loss_evaluator}  \\\n","        validation accuracy: {validation_accuracy_evaluator}')\n","    print(\n","        f'precision: {round(validation_accuracy_evaluator.precision(), 3)}  \\\n","        recall: {round(validation_accuracy_evaluator.recall(), 3)}  \\\n","        f1: {round(validation_accuracy_evaluator.f1(), 3)}')\n","    # the evaluators have __str__\n","    print(\n","        f'duration: {round(epoch_end_time - epoch_start_time, 2)} s  \\\n","        learning rate: {round(scheduler.get_last_lr()[0], 9)}')\n","    \n","    # if reaching best validation loss, back up the state\n","    if validation_loss_evaluator.is_best():\n","        best_epoch = current_epoch\n","        best_loss = validation_loss_evaluator.value()\n","        if save_sd and current_epoch > 1:\n","            # remove existing tempfile if exists\n","            os.remove(state_dict_backup_path) if os.path.isfile(state_dict_backup_path) else None\n","            torch.save(model.state_dict(), state_dict_backup_path)\n","    \n","    # step early stopper\n","    earlystopper.step(validation_loss_evaluator.value())\n","    print('-'*75)\n","    if earlystopper.stop() and epoch >= min_epoches:\n","        print('early stopper triggered, break')\n","        break\n","    if signalstopper.stop():\n","        print('signalled stop')\n","        break\n","\n","signalstopper.reset()\n","# update the starting epoch number\n","starting_epoch = current_epoch\n","print('-' * 75)\n","if best_epoch is not None:\n","    print(f'best validation loss: {best_loss}, epoch {best_epoch}')\n","    print(f'saved to temp file: {state_dict_backup_path}')\n","else:\n","    print(f'validation loss does not improve')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["print(validation_accuracy_evaluator.labels_pred)\n","print(validation_accuracy_evaluator.labels_actual)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([8, 3, 224, 224])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["images.shape"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.float64, torch.float32)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["images.dtype, labels.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from dataio.transforms import grayscale_to_rgb\n","images_rgb = grayscale_to_rgb(images[0])\n","images_rgb.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_dataset = dataloader_dict.get('training_dataset')\n","training_dataset._use_3_channels"]},{"cell_type":"markdown","metadata":{},"source":["save state dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save model state_dicts to target path\n","sd_save_filename = f\"resnet18_state_{datetime.datetime.now().strftime('%m-%d')}-ep_{current_epoch}.pkl\"\n","sd_save_path = os.path.join(bin_path, sd_save_filename)\n","torch.save(model.state_dict(), sd_save_path)\n","print(f'state dict saved to path: {sd_save_path}')"]},{"cell_type":"markdown","metadata":{},"source":["*(debugging the network)*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.drop_rate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(validation_loss_evaluator.labels_actual_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datetime\n","state_dict_save_path = f\"bin/res18_state_{datetime.datetime.now().strftime('%m-%d')}.pkl\"\n","torch.save(model.state_dict(), state_dict_save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_accuracy_evaluator._tfpn()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sum([len(training_dataloader), len(validation_dataloader), len(test_dataloader)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_pred_labels = [torch.argmax(lbl).item() for lbl in validation_accuracy_evaluator.labels_pred_raw]\n","validation_actual_labels = [torch.argmax(lbl).item() for lbl in validation_accuracy_evaluator.labels_actual_raw]\n","sum(validation_pred_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_total = len(validation_actual_labels)\n","n_correct = 0\n","for pred_label, actual_label in zip(validation_pred_labels, validation_actual_labels):\n","    if pred_label == actual_label:\n","        n_correct += 1\n","n_correct / n_total"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_accuracy_evaluator.labels_pred_raw[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_accuracy_evaluator.accuracy()"]},{"cell_type":"markdown","metadata":{},"source":["calculate test set loss and accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# evaluators for test loss and accuracy\n","test_loss_evaluator = LossEvaluator(criterion=loss_function)\n","test_accuracy_evaluator = AccuracyEvaluator()\n","\n","# iterate through test dataloader\n","for images, labels in test_dataloader:\n","    images = images.to(device=device)\n","    labels = labels.to(device=device)\n","    labels_pred = model(images)\n","    loss = loss_function(labels_pred, labels)\n","    test_loss_evaluator.append_loss(loss)\n","    test_accuracy_evaluator.append(labels_pred, labels)\n","print(f'test loss: {test_loss_evaluator}\\\n","    test accuracy: {test_accuracy_evaluator}')\n","print(\n","    f'precision: {round(test_accuracy_evaluator.precision(), 2)}  \\\n","    recall: {round(test_accuracy_evaluator.recall(), 2)}  \\\n","    f1: {round(test_accuracy_evaluator.f1(), 2)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# function shows saliency map\n","def calculate_saliency_map(model, image, target_class):\n","\n","    \"\"\"\n","    Calculates the saliency map for a given image and target class.\n","\n","    Args:\n","    model: A PyTorch model.\n","    image: A PyTorch tensor containing the image.\n","    target_class: The target class.\n","\n","    Returns:\n","    numpy array containing the saliency map.\n","    \"\"\"\n","\n","    # Forward pass the image through the model.\n","    output = model(image)\n","    # Get the gradient of the output with respect to the input image.\n","    # gradient = torch.autograd.grad(output[0][target_class], image[0])\n","    gradient = torch.autograd.grad(\n","        output, image, grad_outputs=target_class.view(1,-1),\n","        allow_unused=True)[0][0]\n","    # Calculate the absolute value of the gradient.\n","    # saliency_map = torch.abs(gradient)\n","    saliency_map = torch.norm(gradient)\n","    # Normalize the saliency map.\n","    saliency_map = saliency_map / torch.max(saliency_map)\n","    saliency_map_np = saliency_map.detach().numpy()\n","    return saliency_map_np"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gradient = torch.autograd.grad(output, image, grad_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","image = images[0][0]\n","plt.imshow(image, cmap='binary_r')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tempfile\n","for images in test_dataloader.view()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(validation_accuracy_evaluator.labels_actual_raw)"]},{"cell_type":"markdown","metadata":{},"source":["## Appendix\n","additional details about the model"]},{"cell_type":"markdown","metadata":{},"source":["structure of the resnet_18 model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["isinstance(model, torch.nn.Module)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
